batch_size: 32
epochs: 1
eval_every_n_epochs: 1
fine_tune_from: None
log_every_n_steps: 50
gpu: cuda:0
vocab_path: 'tokenizer/vocab_full.txt'
cuda: True

optim:
  optimizer: Adam
  init_lr: 0.00001
  weight_decay: 1e-6

model_cgcnn: 
  atom_fea_len: 64
  h_fea_len: 512
  n_conv: 3
  n_h: 1

graph_dataset:
  root_dir: cif_toy   # change here to use cif
  max_num_nbr: 12
  radius: 8
  dmin: 0
  step: 0.2
  random_seed: 666

dataloader:
  val_ratio: 0.5   # Please change here if you using the entire cif files, change to 0.05 as mentioned in SI of the paper
  num_workers: 0

barlow_loss:
  embed_size: 512
  lambd: 0.0051


celoss:
  epsilon: 0.5

Transformer:
  ntoken: 4021
  d_model: 512
  nhead: 8
  d_hid: 512
  nlayers: 6
  dropout: 0.1
